{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "# from torchvision.models import resnet50, vit_b_16\n",
    "# from torchvision import transforms\n",
    "# from torchmetrics import Accuracy\n",
    "from torchinfo import summary\n",
    "\n",
    "from utilities import AITEX\n",
    "from model_architectures import VAE, MiniUNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "defect_codes = {\n",
    "    0: \"Normal\",\n",
    "    2:\t\"Broken end\",\n",
    "    6:\t\"Broken yarn\",\n",
    "    10:\t\"Broken pick\",\n",
    "    16:\t\"Weft curling\",\n",
    "    19:\t\"Fuzzyball\",\n",
    "    22:\t\"Cut selvage\",\n",
    "    23:\t\"Crease\",\n",
    "    25:\t\"Warp ball\",\n",
    "    27:\t\"Knots\",\n",
    "    29:\t\"Contamination\",\n",
    "    30: \"Nep\",\n",
    "    36:\t\"Weft crack\",\n",
    "}\n",
    "\n",
    "class AITEXPatched(AITEX):\n",
    "    def __init__(self, *args, normal_only=False, defect_only=False, **kwargs,):\n",
    "        super(AITEXPatched, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.patched_images = []\n",
    "        self.patched_masks = []\n",
    "        self.has_defect = []\n",
    "        for index, img in enumerate(self.images):\n",
    "            img_new = cv2.resize(img, (4096, 256)) / 255. \n",
    "            self.patched_images.extend([img_new[:,i:i+256] for i in range(0, 4096, 256)])\n",
    "\n",
    "            mask_new = cv2.resize(self.masks[index], (4096, 256))\n",
    "            mask_patches = [mask_new[:,i:i+256] for i in range(0, 4096, 256)]\n",
    "            self.patched_masks.extend(mask_patches)\n",
    "\n",
    "            self.has_defect.extend([1 if np.sum(x) > 0 else 0 for x in mask_patches])\n",
    "        \n",
    "        if normal_only:\n",
    "            indices = [x for x, y in enumerate(self.has_defect) if y==0]\n",
    "            self.patched_images = [self.patched_images[x] for x in indices]\n",
    "            self.patched_masks = [self.patched_masks[x] for x in indices]\n",
    "            self.has_defect = [self.has_defect[x] for x in indices]\n",
    "        if defect_only:\n",
    "            indices = [x for x, y in enumerate(self.has_defect) if y!=0]\n",
    "            self.patched_images = [self.patched_images[x] for x in indices]\n",
    "            self.patched_masks = [self.patched_masks[x] for x in indices]\n",
    "            self.has_defect = [self.has_defect[x] for x in indices]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Get length of full dataset.\"\"\"\n",
    "        return len(self.patched_images)    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return specific index of dataset.\"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img = self.patched_images[idx]\n",
    "        \n",
    "        return np.stack([img, img, img], axis=0), self.has_defect[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "data_dir = os.path.join(root, \"data\")\n",
    "aitex_dir = os.path.join(data_dir, \"aitex\")\n",
    "\n",
    "data = AITEXPatched(aitex_dir, greyscale=True)#, normal_only=True)\n",
    "num_samples = len(data)\n",
    "train_samples = int(num_samples * 0.9)\n",
    "val_samples = num_samples - train_samples\n",
    "train, val = random_split(data, [train_samples, val_samples])\n",
    "\n",
    "# bs = 32\n",
    "# train_loader = DataLoader(train, batch_size=bs, shuffle=True)\n",
    "# val_loader = DataLoader(val, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\compute\\envs\\fdd\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Jad Raad\\AppData\\Local\\Temp\\ipykernel_31380\\1585744307.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n",
      "c:\\compute\\envs\\fdd\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "labels = {0: \"normal\", 1: \"defect\"}\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'google/vit-base-patch16-224', \n",
    "    num_labels=2,\n",
    "    id2label=labels, \n",
    "    label2id={y: x for x, y in labels.items()},\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    inputs = feature_extractor(example[0], return_tensors=\"pt\")\n",
    "    inputs['pixel_values'] = inputs['pixel_values'].reshape((3, 224, 224))\n",
    "    inputs[\"label\"] = example[1]\n",
    "    return inputs\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }\n",
    "\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n",
    "\n",
    "train_ds = [process_example(x) for x in train]\n",
    "val_ds = [process_example(x) for x in val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit-base-beans\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=4,\n",
    "  fp16=True,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/444 [07:56<?, ?it/s]\n",
      "c:\\compute\\envs\\fdd\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "c:\\compute\\envs\\fdd\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "c:\\compute\\envs\\fdd\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [03:17<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2558, 'learning_rate': 0.0001954954954954955, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [03:20<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2379, 'learning_rate': 0.000190990990990991, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [03:23<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1623, 'learning_rate': 0.0001864864864864865, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [03:26<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2618, 'learning_rate': 0.000181981981981982, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [03:29<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1765, 'learning_rate': 0.0001774774774774775, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [03:32<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.232, 'learning_rate': 0.000172972972972973, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [03:35<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2027, 'learning_rate': 0.00016846846846846846, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [03:38<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.182, 'learning_rate': 0.00016396396396396395, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [03:41<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1743, 'learning_rate': 0.00015945945945945947, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [03:44<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2311, 'learning_rate': 0.00015495495495495496, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [03:46<01:40,  3.41it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1840788871049881, 'eval_accuracy': 0.9695431472081218, 'eval_runtime': 1.911, 'eval_samples_per_second': 206.174, 'eval_steps_per_second': 13.082, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\compute\\envs\\fdd\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "c:\\compute\\envs\\fdd\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [03:51<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.16, 'learning_rate': 0.00015045045045045046, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [03:53<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2694, 'learning_rate': 0.00014594594594594595, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [03:56<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2053, 'learning_rate': 0.00014144144144144144, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:00<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1654, 'learning_rate': 0.00013693693693693693, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:03<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1847, 'learning_rate': 0.00013243243243243243, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:06<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2323, 'learning_rate': 0.00012792792792792795, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:09<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1755, 'learning_rate': 0.00012342342342342344, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:12<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1418, 'learning_rate': 0.00011891891891891893, 'epoch': 1.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:15<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2526, 'learning_rate': 0.00011441441441441443, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:18<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.169, 'learning_rate': 0.00010990990990990993, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:20<01:40,  3.41it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1403149962425232, 'eval_accuracy': 0.9695431472081218, 'eval_runtime': 2.122, 'eval_samples_per_second': 185.674, 'eval_steps_per_second': 11.781, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\compute\\envs\\fdd\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "c:\\compute\\envs\\fdd\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:24<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2644, 'learning_rate': 0.0001054054054054054, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:27<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1742, 'learning_rate': 0.00010090090090090089, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:30<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2546, 'learning_rate': 9.639639639639641e-05, 'epoch': 2.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:33<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1735, 'learning_rate': 9.18918918918919e-05, 'epoch': 2.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:36<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2293, 'learning_rate': 8.738738738738738e-05, 'epoch': 2.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:40<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2346, 'learning_rate': 8.288288288288289e-05, 'epoch': 2.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:43<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1699, 'learning_rate': 7.837837837837838e-05, 'epoch': 2.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:46<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2275, 'learning_rate': 7.387387387387387e-05, 'epoch': 2.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:49<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1723, 'learning_rate': 6.936936936936938e-05, 'epoch': 2.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:52<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1507, 'learning_rate': 6.486486486486487e-05, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\u001b[A                                              \n",
      "\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:54<01:40,  3.41it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13638249039649963, 'eval_accuracy': 0.9695431472081218, 'eval_runtime': 1.801, 'eval_samples_per_second': 218.767, 'eval_steps_per_second': 13.881, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\compute\\envs\\fdd\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "c:\\compute\\envs\\fdd\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [04:58<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2365, 'learning_rate': 6.0360360360360365e-05, 'epoch': 2.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [05:01<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1906, 'learning_rate': 5.585585585585585e-05, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [05:04<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1906, 'learning_rate': 5.135135135135135e-05, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [05:07<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1605, 'learning_rate': 4.684684684684685e-05, 'epoch': 3.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [05:11<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2042, 'learning_rate': 4.234234234234234e-05, 'epoch': 3.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [05:14<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1806, 'learning_rate': 3.783783783783784e-05, 'epoch': 3.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [05:17<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1809, 'learning_rate': 3.3333333333333335e-05, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [05:20<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1593, 'learning_rate': 2.882882882882883e-05, 'epoch': 3.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [05:23<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2037, 'learning_rate': 2.4324324324324327e-05, 'epoch': 3.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [05:26<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2108, 'learning_rate': 1.981981981981982e-05, 'epoch': 3.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\u001b[A                                              \n",
      "\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [05:28<01:40,  3.41it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13753864169120789, 'eval_accuracy': 0.9695431472081218, 'eval_runtime': 2.067, 'eval_samples_per_second': 190.614, 'eval_steps_per_second': 12.095, 'epoch': 3.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\compute\\envs\\fdd\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "c:\\compute\\envs\\fdd\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [05:33<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2383, 'learning_rate': 1.5315315315315316e-05, 'epoch': 3.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [05:36<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1711, 'learning_rate': 1.0810810810810812e-05, 'epoch': 3.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [05:39<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2704, 'learning_rate': 6.306306306306306e-06, 'epoch': 3.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 100/444 [05:42<01:40,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.119, 'learning_rate': 1.801801801801802e-06, 'epoch': 3.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 444/444 [02:29<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 149.195, 'train_samples_per_second': 94.963, 'train_steps_per_second': 2.976, 'train_loss': 0.2015069006262599, 'epoch': 4.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        4.0\n",
      "  train_loss               =     0.2015\n",
      "  train_runtime            = 0:02:29.19\n",
      "  train_samples_per_second =     94.963\n",
      "  train_steps_per_second   =      2.976\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_runtime': 149.195,\n",
       " 'train_samples_per_second': 94.963,\n",
       " 'train_steps_per_second': 2.976,\n",
       " 'train_loss': 0.2015069006262599,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results.metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fdd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

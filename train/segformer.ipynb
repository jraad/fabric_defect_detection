{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import SegformerFeatureExtractor\n",
    "from torchvision import transforms\n",
    "\n",
    "from utilities import AITEXPatchedSegmentation\n",
    "from model_architectures import BinaryClassifier, MiniUNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSegmentationDataset(AITEXPatchedSegmentation):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, feature_extractor, **kwargs,):\n",
    "        super(SemanticSegmentationDataset, self).__init__(*args, **kwargs)\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.defect_images[idx]\n",
    "        mask = self.defect_masks[idx]\n",
    "\n",
    "        encoded_inputs = self.feature_extractor(image.expand(3, 256, 256), mask, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_()\n",
    "\n",
    "        return encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "model_dir = os.path.join(root, \"models\")\n",
    "data_dir = os.path.join(root, \"data\")\n",
    "aitex_dir = os.path.join(data_dir, \"aitex\")\n",
    "transform = transforms.Compose([])\n",
    "\n",
    "data = SemanticSegmentationDataset(aitex_dir, feature_extractor=feature_extractor, transform=transform)\n",
    "\n",
    "bs = 4\n",
    "train = DataLoader(data, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\compute\\envs\\fdd\\lib\\site-packages\\transformers\\models\\segformer\\feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  warnings.warn(\n",
      "c:\\compute\\envs\\fdd\\lib\\site-packages\\transformers\\models\\segformer\\image_processing_segformer.py:99: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "feature_extractor.do_reduce_labels = False\n",
    "feature_extractor.size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([1, 256, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/segformer-b0-finetuned-ade-512-512\", \n",
    "    return_dict=False, \n",
    "    num_labels=1,\n",
    "    # id2label=self.id2label,\n",
    "    # label2id=self.label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_iou': 0.97845458984375,\n",
       " 'mean_accuracy': 1.0,\n",
       " 'overall_accuracy': 1.0,\n",
       " 'per_category_iou': array([0.97845459]),\n",
       " 'per_category_accuracy': array([1.])}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "train_mean_iou = load_metric(\"mean_iou\")\n",
    "\n",
    "x = data[0]['pixel_values'].reshape(1, 3, 128, 128)\n",
    "y = data[0]['labels'].reshape(1, 128, 128)\n",
    "outputs = model(pixel_values=x, labels=y)\n",
    "loss, logits = outputs[0], outputs[1]\n",
    "\n",
    "upsampled_logits = nn.functional.interpolate(\n",
    "    logits, \n",
    "    size=y.shape[-2:], \n",
    "    mode=\"bilinear\", \n",
    "    align_corners=False\n",
    ")\n",
    "\n",
    "predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "train_mean_iou.add_batch(\n",
    "    predictions=predicted.detach().cpu().numpy(), \n",
    "    references=y.detach().cpu().numpy()\n",
    ")\n",
    "train_mean_iou.compute(\n",
    "    num_labels=1, \n",
    "    ignore_index=255, \n",
    "    reduce_labels=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 0.04712343215942383\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 1\n",
      "Loss: 0.056149400770664215\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 2\n",
      "Loss: 0.057191986590623856\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 3\n",
      "Loss: 0.0935225635766983\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 4\n",
      "Loss: 0.04893092066049576\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 5\n",
      "Loss: 0.05527281016111374\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 6\n",
      "Loss: 0.05002467334270477\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 7\n",
      "Loss: 0.06600642204284668\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 8\n",
      "Loss: 0.0592881515622139\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 9\n",
      "Loss: 0.051115769892930984\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 10\n",
      "Loss: 0.05140042304992676\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 11\n",
      "Loss: 0.05138517916202545\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 12\n",
      "Loss: 0.04870535433292389\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 13\n",
      "Loss: 0.04892546311020851\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 14\n",
      "Loss: 0.04975645989179611\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 15\n",
      "Loss: 0.04909235239028931\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 16\n",
      "Loss: 0.04791451618075371\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 17\n",
      "Loss: 0.04899904504418373\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 18\n",
      "Loss: 0.04824615269899368\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 19\n",
      "Loss: 0.04825893044471741\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 20\n",
      "Loss: 0.04771387577056885\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 21\n",
      "Loss: 0.048829276114702225\n",
      "Mean_iou: 0.9598319388724662\n",
      "Mean accuracy: 1.0\n",
      "Epoch: 22\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Jad Raad\\Documents\\proj\\code_projects\\loopr\\fabric_defect_detection\\train\\segformer.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jad%20Raad/Documents/proj/code_projects/loopr/fabric_defect_detection/train/segformer.ipynb#W2sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m       predicted \u001b[39m=\u001b[39m upsampled_logits\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jad%20Raad/Documents/proj/code_projects/loopr/fabric_defect_detection/train/segformer.ipynb#W2sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m       \u001b[39m# note that the metric expects predictions + labels as numpy arrays\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Jad%20Raad/Documents/proj/code_projects/loopr/fabric_defect_detection/train/segformer.ipynb#W2sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m       metric\u001b[39m.\u001b[39;49madd_batch(predictions\u001b[39m=\u001b[39;49mpredicted\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy(), references\u001b[39m=\u001b[39;49mlabels\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jad%20Raad/Documents/proj/code_projects/loopr/fabric_defect_detection/train/segformer.ipynb#W2sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m metrics \u001b[39m=\u001b[39m metric\u001b[39m.\u001b[39mcompute(num_labels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jad%20Raad/Documents/proj/code_projects/loopr/fabric_defect_detection/train/segformer.ipynb#W2sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m                         ignore_index\u001b[39m=\u001b[39m\u001b[39m255\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jad%20Raad/Documents/proj/code_projects/loopr/fabric_defect_detection/train/segformer.ipynb#W2sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m                         reduce_labels\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m# we've already reduced the labels before)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jad%20Raad/Documents/proj/code_projects/loopr/fabric_defect_detection/train/segformer.ipynb#W2sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jad%20Raad/Documents/proj/code_projects/loopr/fabric_defect_detection/train/segformer.ipynb#W2sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss:\u001b[39m\u001b[39m\"\u001b[39m, loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\compute\\envs\\fdd\\lib\\site-packages\\datasets\\metric.py:498\u001b[0m, in \u001b[0;36mMetric.add_batch\u001b[1;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_writer()\n\u001b[0;32m    497\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 498\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwriter\u001b[39m.\u001b[39;49mwrite_batch(batch)\n\u001b[0;32m    499\u001b[0m \u001b[39mexcept\u001b[39;00m pa\u001b[39m.\u001b[39mArrowInvalid:\n\u001b[0;32m    500\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39mlen\u001b[39m(batch[c]) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(batch\u001b[39m.\u001b[39mvalues()))) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m batch):\n",
      "File \u001b[1;32mc:\\compute\\envs\\fdd\\lib\\site-packages\\datasets\\arrow_writer.py:553\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    551\u001b[0m         col_try_type \u001b[39m=\u001b[39m try_features[col] \u001b[39mif\u001b[39;00m try_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m col \u001b[39min\u001b[39;00m try_features \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    552\u001b[0m         typed_sequence \u001b[39m=\u001b[39m OptimizedTypedSequence(col_values, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mcol_type, try_type\u001b[39m=\u001b[39mcol_try_type, col\u001b[39m=\u001b[39mcol)\n\u001b[1;32m--> 553\u001b[0m         arrays\u001b[39m.\u001b[39mappend(pa\u001b[39m.\u001b[39;49marray(typed_sequence))\n\u001b[0;32m    554\u001b[0m         inferred_features[col] \u001b[39m=\u001b[39m typed_sequence\u001b[39m.\u001b[39mget_inferred_type()\n\u001b[0;32m    555\u001b[0m schema \u001b[39m=\u001b[39m inferred_features\u001b[39m.\u001b[39marrow_schema \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema\n",
      "File \u001b[1;32mc:\\compute\\envs\\fdd\\lib\\site-packages\\pyarrow\\array.pxi:243\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\compute\\envs\\fdd\\lib\\site-packages\\pyarrow\\array.pxi:110\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\compute\\envs\\fdd\\lib\\site-packages\\datasets\\arrow_writer.py:204\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[39m# otherwise we can finally use the user's type\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m         \u001b[39m# We use cast_array_to_feature to support casting to custom types like Audio and Image\u001b[39;00m\n\u001b[0;32m    202\u001b[0m         \u001b[39m# Also, when trying type \"string\", we don't want to convert integers or floats to \"string\".\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         \u001b[39m# We only do it if trying_type is False - since this is what the user asks for.\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m         out \u001b[39m=\u001b[39m cast_array_to_feature(out, \u001b[39mtype\u001b[39;49m, allow_number_to_str\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrying_type)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m    206\u001b[0m \u001b[39mexcept\u001b[39;00m (\n\u001b[0;32m    207\u001b[0m     \u001b[39mTypeError\u001b[39;00m,\n\u001b[0;32m    208\u001b[0m     pa\u001b[39m.\u001b[39mlib\u001b[39m.\u001b[39mArrowInvalid,\n\u001b[0;32m    209\u001b[0m     pa\u001b[39m.\u001b[39mlib\u001b[39m.\u001b[39mArrowNotImplementedError,\n\u001b[0;32m    210\u001b[0m ) \u001b[39mas\u001b[39;00m e:  \u001b[39m# handle type errors and overflows\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[39m# Ignore ArrowNotImplementedError caused by trying type, otherwise re-raise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\compute\\envs\\fdd\\lib\\site-packages\\datasets\\table.py:1833\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[1;34m(array, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1831\u001b[0m     \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mchunked_array([func(chunk, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m array\u001b[39m.\u001b[39mchunks])\n\u001b[0;32m   1832\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1833\u001b[0m     \u001b[39mreturn\u001b[39;00m func(array, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\compute\\envs\\fdd\\lib\\site-packages\\datasets\\table.py:2091\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[1;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[0;32m   2089\u001b[0m         \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mFixedSizeListArray\u001b[39m.\u001b[39mfrom_arrays(_c(array\u001b[39m.\u001b[39mvalues, feature\u001b[39m.\u001b[39mfeature), feature\u001b[39m.\u001b[39mlength)\n\u001b[0;32m   2090\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2091\u001b[0m     casted_values \u001b[39m=\u001b[39m _c(array\u001b[39m.\u001b[39;49mvalues, feature\u001b[39m.\u001b[39;49mfeature)\n\u001b[0;32m   2092\u001b[0m     \u001b[39mif\u001b[39;00m casted_values\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m array\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mtype:\n\u001b[0;32m   2093\u001b[0m         \u001b[39mreturn\u001b[39;00m array\n",
      "File \u001b[1;32mc:\\compute\\envs\\fdd\\lib\\site-packages\\datasets\\table.py:1833\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[1;34m(array, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1831\u001b[0m     \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mchunked_array([func(chunk, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m array\u001b[39m.\u001b[39mchunks])\n\u001b[0;32m   1832\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1833\u001b[0m     \u001b[39mreturn\u001b[39;00m func(array, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\compute\\envs\\fdd\\lib\\site-packages\\datasets\\table.py:2091\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[1;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[0;32m   2089\u001b[0m         \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mFixedSizeListArray\u001b[39m.\u001b[39mfrom_arrays(_c(array\u001b[39m.\u001b[39mvalues, feature\u001b[39m.\u001b[39mfeature), feature\u001b[39m.\u001b[39mlength)\n\u001b[0;32m   2090\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2091\u001b[0m     casted_values \u001b[39m=\u001b[39m _c(array\u001b[39m.\u001b[39;49mvalues, feature\u001b[39m.\u001b[39;49mfeature)\n\u001b[0;32m   2092\u001b[0m     \u001b[39mif\u001b[39;00m casted_values\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m array\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mtype:\n\u001b[0;32m   2093\u001b[0m         \u001b[39mreturn\u001b[39;00m array\n",
      "File \u001b[1;32mc:\\compute\\envs\\fdd\\lib\\site-packages\\datasets\\table.py:1833\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[1;34m(array, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1831\u001b[0m     \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mchunked_array([func(chunk, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m array\u001b[39m.\u001b[39mchunks])\n\u001b[0;32m   1832\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1833\u001b[0m     \u001b[39mreturn\u001b[39;00m func(array, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\compute\\envs\\fdd\\lib\\site-packages\\datasets\\table.py:2139\u001b[0m, in \u001b[0;36mcast_array_to_feature\u001b[1;34m(array, feature, allow_number_to_str)\u001b[0m\n\u001b[0;32m   2137\u001b[0m     \u001b[39mreturn\u001b[39;00m array_cast(array, get_nested_type(feature), allow_number_to_str\u001b[39m=\u001b[39mallow_number_to_str)\n\u001b[0;32m   2138\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(feature, (Sequence, \u001b[39mdict\u001b[39m, \u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m-> 2139\u001b[0m     \u001b[39mreturn\u001b[39;00m array_cast(array, feature(), allow_number_to_str\u001b[39m=\u001b[39;49mallow_number_to_str)\n\u001b[0;32m   2140\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt cast array of type\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00marray\u001b[39m.\u001b[39mtype\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mto\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfeature\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\compute\\envs\\fdd\\lib\\site-packages\\datasets\\table.py:1833\u001b[0m, in \u001b[0;36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[1;34m(array, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1831\u001b[0m     \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39mchunked_array([func(chunk, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m array\u001b[39m.\u001b[39mchunks])\n\u001b[0;32m   1832\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1833\u001b[0m     \u001b[39mreturn\u001b[39;00m func(array, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\compute\\envs\\fdd\\lib\\site-packages\\datasets\\table.py:2025\u001b[0m, in \u001b[0;36marray_cast\u001b[1;34m(array, pa_type, allow_number_to_str)\u001b[0m\n\u001b[0;32m   2023\u001b[0m     \u001b[39mif\u001b[39;00m pa\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_null(pa_type) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m pa\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mis_null(array\u001b[39m.\u001b[39mtype):\n\u001b[0;32m   2024\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt cast array of type \u001b[39m\u001b[39m{\u001b[39;00marray\u001b[39m.\u001b[39mtype\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mpa_type\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2025\u001b[0m     \u001b[39mreturn\u001b[39;00m array\u001b[39m.\u001b[39;49mcast(pa_type)\n\u001b[0;32m   2026\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt cast array of type\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00marray\u001b[39m.\u001b[39mtype\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mto\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mpa_type\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\compute\\envs\\fdd\\lib\\site-packages\\pyarrow\\array.pxi:935\u001b[0m, in \u001b[0;36mpyarrow.lib.Array.cast\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\compute\\envs\\fdd\\lib\\site-packages\\pyarrow\\compute.py:400\u001b[0m, in \u001b[0;36mcast\u001b[1;34m(arr, target_type, safe, options, memory_pool)\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m         options \u001b[39m=\u001b[39m CastOptions\u001b[39m.\u001b[39msafe(target_type)\n\u001b[1;32m--> 400\u001b[0m \u001b[39mreturn\u001b[39;00m call_function(\u001b[39m\"\u001b[39;49m\u001b[39mcast\u001b[39;49m\u001b[39m\"\u001b[39;49m, [arr], options, memory_pool)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
    "# move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "metric = load_metric(\"mean_iou\")\n",
    "model.train()\n",
    "for epoch in range(200):  # loop over the dataset multiple times\n",
    "   print(\"Epoch:\", epoch)\n",
    "   for batch in data:\n",
    "      # get the inputs;\n",
    "      pixel_values = batch[\"pixel_values\"].reshape(1, 3, 128, 128).to(device)\n",
    "      labels = batch[\"labels\"].reshape(1, 128, 128).to(device)\n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "      loss, logits = outputs\n",
    "      \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # evaluate\n",
    "      with torch.no_grad():\n",
    "         upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "         predicted = upsampled_logits.argmax(dim=1)\n",
    "         \n",
    "         # note that the metric expects predictions + labels as numpy arrays\n",
    "         metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
    "         \n",
    "   metrics = metric.compute(num_labels=1, \n",
    "                           ignore_index=255,\n",
    "                           reduce_labels=False, # we've already reduced the labels before)\n",
    "   )\n",
    "\n",
    "   print(\"Loss:\", loss.item())\n",
    "   print(\"Mean_iou:\", metrics[\"mean_iou\"])\n",
    "   print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fdd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
